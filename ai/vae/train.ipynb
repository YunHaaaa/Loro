{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torchaudio\n",
    "Resample = torchaudio.transforms.Resample(44100, 48000, resampling_method='kaiser_window')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    my_cuda = 1\n",
    "else: \n",
    "    device = 'cpu'\n",
    "    my_cuda = 0\n",
    "    \n",
    "Resample = Resample.to(device)\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy import interpolate as sp_interpolate\n",
    "import json\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import soundfile as sf\n",
    "# import sounddevice as sd\n",
    "import configparser\n",
    "import random\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 44100\n",
    "sr = sampling_rate\n",
    "\n",
    "hop_length = 128\n",
    "\n",
    "segment_length = 1024\n",
    "n_units = 2048\n",
    "latent_dim = 256\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "audio_fold = Path(r'../vits2_pytorch/data/filelists/lolo_audio_sid_text_test_filelist.txt')\n",
    "audio = audio_fold\n",
    "lts_audio_files = [f for f in audio_fold.glob('*.wav')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Following should give you more than 0. Otherwise, the dataset is not in the right place. Please make sure that the following folder is there: rawaudiovae/content/2022-zkm-workshop\n",
    "\n",
    "len(lts_audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "\n",
    "class raw_VAE(nn.Module):\n",
    "  def __init__(self, segment_length, n_units, latent_dim):\n",
    "    super(raw_VAE, self).__init__()\n",
    "\n",
    "    self.segment_length = segment_length\n",
    "    self.n_units = n_units\n",
    "    self.latent_dim = latent_dim\n",
    "    \n",
    "    self.fc1 = nn.Linear(segment_length, n_units)\n",
    "    self.fc21 = nn.Linear(n_units, latent_dim)\n",
    "    self.fc22 = nn.Linear(n_units, latent_dim)\n",
    "    self.fc3 = nn.Linear(latent_dim, n_units)\n",
    "    self.fc4 = nn.Linear(n_units, segment_length)\n",
    "\n",
    "  def encode(self, x):\n",
    "      h1 = F.relu(self.fc1(x))\n",
    "      return self.fc21(h1), self.fc22(h1)\n",
    "\n",
    "  def reparameterize(self, mu, logvar):\n",
    "      std = torch.exp(0.5*logvar)\n",
    "      eps = torch.randn_like(std)\n",
    "      return mu + eps*std\n",
    "\n",
    "  def decode(self, z):\n",
    "      h3 = F.relu(self.fc3(z))\n",
    "      return F.tanh(self.fc4(h3))\n",
    "\n",
    "  def forward(self, x):\n",
    "      mu, logvar = self.encode(x.view(-1, self.segment_length))\n",
    "      z = self.reparameterize(mu, logvar)\n",
    "      return self.decode(z), mu, logvar\n",
    "\n",
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(recon_x, x, mu, logvar, kl_beta, segment_length):\n",
    "  recon_loss = F.mse_loss(recon_x, x.view(-1, segment_length))\n",
    "\n",
    "  # see Appendix B from VAE paper:\n",
    "  # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "  # https://arxiv.org/abs/1312.6114\n",
    "  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "  KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "  return recon_loss + ( kl_beta * KLD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets \n",
    "\n",
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, audio_np, segment_length, sampling_rate, hop_size, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        self.hop_size = hop_size\n",
    "        \n",
    "        if segment_length % hop_size != 0:\n",
    "            raise ValueError(\"segment_length {} is not a multiple of hop_size {}\".format(segment_length, hop_size))\n",
    "\n",
    "        if len(audio_np) % hop_size != 0:\n",
    "            num_zeros = hop_size - (len(audio_np) % hop_size)\n",
    "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
    "\n",
    "        self.audio_np = audio_np\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Take segment\n",
    "        seg_start = index * self.hop_size\n",
    "        seg_end = (index * self.hop_size) + self.segment_length\n",
    "        sample = self.audio_np[ seg_start : seg_end ]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.audio_np) // self.hop_size) - (self.segment_length // self.hop_size) + 1\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        return torch.from_numpy(sample)\n",
    "\n",
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This is the main class that calculates the spectrogram and returns the\n",
    "    spectrogram, audio pair.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, audio_np, segment_length, sampling_rate, transform=None):\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.segment_length = segment_length\n",
    "        \n",
    "        if len(audio_np) % segment_length != 0:\n",
    "            num_zeros = segment_length - (len(audio_np) % segment_length)\n",
    "            audio_np = np.pad(audio_np, (0, num_zeros), 'constant', constant_values=(0,0))\n",
    "\n",
    "        self.audio_np = audio_np\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Take segment\n",
    "        seg_start = index * self.segment_length\n",
    "        seg_end = (index * self.segment_length) + self.segment_length\n",
    "        sample = self.audio_np[ seg_start : seg_end ]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_np) // self.segment_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.load(Path(r'./content/2022-zkm-workshop/nospectral/erokia/spectralvae/run-000/checkpoints/ckpt_00500'), map_location=torch.device(device))\n",
    "if my_cuda:\n",
    "    raw_model = raw_VAE(segment_length, n_units, latent_dim).to(device)\n",
    "else:\n",
    "    raw_model = raw_VAE(segment_length, n_units, latent_dim)\n",
    "raw_model.load_state_dict(state['state_dict'])\n",
    "raw_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audio_1_path = lts_audio_files[random.randint(0, len(lts_audio_files) - 1)]\n",
    "test_audio_1, fs = librosa.load(test_audio_1_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(test_audio_1, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_audio_2_path = lts_audio_files[random.randint(0, len(lts_audio_files)- 1)]\n",
    "test_audio_2, fs = librosa.load(test_audio_2_path, sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(test_audio_2, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should match the audio lengths\n",
    "# 0 is crop the longer, \n",
    "# 1 is repeat the shorter, \n",
    "#  \n",
    "\n",
    "# TODO make this an array/tensor of two - check if padding functions have these \n",
    "\n",
    "match_size = 1\n",
    "\n",
    "if match_size == 0:\n",
    "    if test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
    "        test_audio_2 = test_audio_2[:test_audio_1.shape[0]]\n",
    "    else:\n",
    "        test_audio_1 = test_audio_1[:test_audio_2.shape[0]]\n",
    "\n",
    "if match_size == 1:\n",
    "    if test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
    "        while test_audio_1.shape[0] < test_audio_2.shape[0]:\n",
    "            test_audio_1 = np.concatenate((test_audio_1, test_audio_1), 0)\n",
    "        test_audio_1 = test_audio_1[:test_audio_2.shape[0]] \n",
    "\n",
    "    else:\n",
    "        while test_audio_2.shape[0] < test_audio_1.shape[0]:\n",
    "            test_audio_2 = np.concatenate((test_audio_2, test_audio_2), 0)\n",
    "        test_audio_2 = test_audio_2[:test_audio_1.shape[0]]\n",
    "\n",
    "# Create the dataset\n",
    "test_dataset1 = TestDataset(test_audio_1, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "test_dataset2 = TestDataset(test_audio_2, segment_length = segment_length, sampling_rate = sampling_rate, transform=ToTensor())\n",
    "\n",
    "test_dataloader1 = DataLoader(test_dataset1, batch_size = batch_size, shuffle=False)\n",
    "test_dataloader2 = DataLoader(test_dataset2, batch_size = batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_z_dist(test_dataloader, raw_model, device):\n",
    "    init_test = True\n",
    "    for iterno, test_sample in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            test_sample = test_sample.to(device)\n",
    "            test_mu, test_logvar = raw_model.encode(test_sample)\n",
    "\n",
    "        if init_test:\n",
    "            test_z_mu = test_mu \n",
    "            test_z_logvar = test_logvar\n",
    "            init_test = False\n",
    "\n",
    "        else:\n",
    "            test_z_mu = torch.cat((test_z_mu, test_mu ),0)\n",
    "            test_z_logvar = torch.cat((test_z_logvar, test_logvar ),0)\n",
    "    return test_z_mu, test_z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test1_z_mu, test1_z_logvar = raw_to_z_dist(test_dataloader1, raw_model, device)\n",
    "test2_z_mu, test2_z_logvar = raw_to_z_dist(test_dataloader2, raw_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_interpolate_stepwise_z_dist(test1_z_mu, test1_z_logvar, test2_z_mu, test2_z_logvar, interpolation_range, raw_model):\n",
    "\n",
    "    init_test = True\n",
    "    for interpolation in interpolation_range:\n",
    "\n",
    "        inter_z_mu = torch.add( torch.mul(test1_z_mu, (1-interpolation)), torch.mul(test2_z_mu, interpolation) )\n",
    "        inter_z_logvar = torch.add( torch.mul(test1_z_logvar, (1-interpolation)), torch.mul(test2_z_logvar, interpolation) )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            test_pred_z = raw_model.reparameterize(inter_z_mu, inter_z_logvar)\n",
    "            test_pred = raw_model.decode(test_pred_z)\n",
    "\n",
    "        if init_test:\n",
    "            test_predictions = test_pred\n",
    "            init_test = False\n",
    "\n",
    "        else:\n",
    "            test_predictions = torch.cat((test_predictions, test_pred ),0)\n",
    "        \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpolation_range = np.arange(0, 1.1, 0.2)\n",
    "\n",
    "inter_raw_all = raw_interpolate_stepwise_z_dist(test1_z_mu, test1_z_logvar, test2_z_mu, test2_z_logvar, interpolation_range, raw_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_raw_all_np = inter_raw_all.view(-1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Audio(inter_raw_all_np, rate=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path  = './content/my_audio.wav'\n",
    "sf.write(out_path, inter_raw_all_np, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.waveshow(inter_raw_all_np, sr=sr, color=\"magenta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(inter_raw_all_np, hop_length=hop_length)),ref=np.max)\n",
    "img = librosa.display.specshow(D, y_axis='log', sr=sr, hop_length=hop_length, x_axis='time', ax=ax)\n",
    "ax.set(title='Log-frequency power spectrogram')\n",
    "ax.label_outer()\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
